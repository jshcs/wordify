{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from headers import *\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readdict(filename):\n",
    "    print \"Extracting the dictionary from %s\"%(filename)\n",
    "    with open(filename,'rb') as handle:\n",
    "        dictionary=pickle.load(handle)\n",
    "\n",
    "    wcount=len(dictionary)\n",
    "    return dictionary,wcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary,wcount=readdict(WNDICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_swords(ltokens):\n",
    "    res=[w for w in ltokens if w not in SWORDS]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tok_sentence(w,m):\n",
    "    sentence=w+' '+m\n",
    "    tokenized_sentence=sentence.split()\n",
    "    return remove_swords(tokenized_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg=0.0\n",
    "local_vocab=set()\n",
    "for w,m in dictionary:\n",
    "    #print tok_sentence(w,m)\n",
    "    #local_vocab.update(tok_sentence(w,m))\n",
    "    avg+=len(m.split(' '))\n",
    "    local_vocab.add(w)\n",
    "print avg/len(dictionary)\n",
    "print len(local_vocab)\n",
    "print len(global_vocab)\n",
    "if 'blarina'in set(local_vocab):\n",
    "    print True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict2=[x for x in dictionary if nlp(unicode(x[0],'utf-8')).has_vector and nlp(unicode(x[0],'utf-8')).vector.all()!=0]\n",
    "len(dict2)\n",
    "\n",
    "max_sentence=0\n",
    "ms=[]\n",
    "for i in range(len(dict2)):\n",
    "    tok=dict2[i][1].split()\n",
    "    if len(tok)>DEF_LEN:\n",
    "        new_def=' '.join(tok[:DEF_LEN])\n",
    "        dict2[i]=(dict2[i][0],new_def)\n",
    "        #print \"Removed\",len(dict2[i][1].split())\n",
    "    elif len(tok)<DEF_LEN:\n",
    "        diff=DEF_LEN-len(tok)\n",
    "        for x in range(diff):\n",
    "            tok.append('<PAD>')\n",
    "        new_def=' '.join(tok)\n",
    "        dict2[i]=(dict2[i][0],new_def)\n",
    "        #print \"Added\",len(dict2[i][1].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_set=[]\n",
    "X_set=np.zeros((len(dict2),DEF_LEN,WV_SIZE))\n",
    "Y_GT=[]\n",
    "\n",
    "for i in range(len(dict2)):\n",
    "    w=dict2[i][0]\n",
    "    m=dict2[i][1]\n",
    "    #check for the word\n",
    "    docw=nlp(unicode(w,'utf-8'))\n",
    "    Y_set.append(docw[0].vector)\n",
    "    Y_GT.append(w)\n",
    "    #check for all the words in the meaning\n",
    "    tok=m.split()\n",
    "    for t in range(len(tok)):\n",
    "        docm=nlp(unicode(tok[t],'utf-8'))\n",
    "        if docm[0].has_vector and docm[0].vector.all()!=0:\n",
    "            X_set[i,t,:]=docm[0].vector\n",
    "        else:\n",
    "            X_set[i,t,:]=np.zeros((WV_SIZE,))\n",
    "Y_set=np.array(Y_set)\n",
    "print Y_set.shape\n",
    "print len(Y_GT)\n",
    "print X_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_val,X_test,Y_train,Y_val,Y_test=X_set[:-2000],X_set[-2000:-1000],X_set[-1000:],Y_set[:-2000],Y_set[-2000:-1000],Y_set[-1000:]\n",
    "print X_train.shape,X_val.shape,X_test.shape,Y_train.shape,Y_val.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readwe(filename):\n",
    "    with open(filename,'rb') as handle:\n",
    "        nlp=pickle.load(handle)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_whole_file(filename):\n",
    "    dict,wcount=readdict(filename)\n",
    "    whole_thing=''\n",
    "    for word,meaning in dict:\n",
    "        #removed word from the return string\n",
    "        whole_thing+=' '+meaning\n",
    "    return whole_thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeemb(word_embeddings,filename):\n",
    "    with open(filename,'wb') as handle:\n",
    "        pickle.dump(word_embeddings,handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print \"Done writing the word_embeddings to %s\"%(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reademb(filename):\n",
    "    print \"Extracting the word_embeddings from %s\"%(filename)\n",
    "    with open(filename,'rb') as handle:\n",
    "        word_embeddings=pickle.load(handle)\n",
    "\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(dict):\n",
    "    #dict is a list of (word, definitions)\n",
    "    #Every word-definition pair is converted to a sentence by concatenating the word+definition\n",
    "    #This sentence is tokenized to get a list of tokens for every word-definition pair\n",
    "    #All such lists are appended to tok\n",
    "    tok=[]\n",
    "    for i,(w,d) in enumerate(dict):\n",
    "        #removed word from the token list\n",
    "        sentence=w+' '+d\n",
    "        tokenized_sentence=sentence.split()\n",
    "        tok.append(remove_swords(tokenized_sentence))\n",
    "    return tok,len(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(dict,ratio):\n",
    "    N=len(dict)\n",
    "    train_size=ratio\n",
    "    random.shuffle(dict)\n",
    "    train_set=dict[:-train_size]\n",
    "    test_set=list(set(dict)-set(train_set))\n",
    "    return train_set,test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_cumm_embedding(input_sentence,method):\n",
    "    N=WV_SIZE\n",
    "    doc=nlp(input_sentence)\n",
    "    if method=='additive':\n",
    "        res=np.zeros((N,))\n",
    "        for i in range(len(doc)):\n",
    "            if not doc[i].has_vector:\n",
    "                continue\n",
    "            if doc[i].vector.all()!=0:\n",
    "                res+=doc[i].vector\n",
    "    elif method=='multiplicative':\n",
    "        res=np.ones((N,))\n",
    "        for i in range(len(doc)):\n",
    "            if not doc[i].has_vector:\n",
    "                continue\n",
    "            if doc[i].vector.all()!=0:\n",
    "                res*=doc[i].vector\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_sim(word_embedding1,word_embedding2):\n",
    "    d1=np.sqrt(np.sum(word_embedding1**2))\n",
    "    d2=np.sqrt(np.sum(word_embedding2**2))\n",
    "    n=word_embedding1.dot(word_embedding2)\n",
    "    return n/(d1*d2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sq_distance(word_embedding1,word_embedding2):\n",
    "    return tf.sqrt(tf.reduce_sum((word_embedding1-word_embedding2)**2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_most_similar(input_embedding):\n",
    "    cs_list=[]\n",
    "    for v in set(local_vocab):\n",
    "        doc=nlp(unicode(v,'utf-8'))\n",
    "        if doc[0].has_vector:\n",
    "            if doc[0].vector.all()!=0.0:\n",
    "                cs=cosine_sim(input_embedding,doc[0].vector)\n",
    "                if not math.isnan(cs):\n",
    "                    cs_list.append((doc[0].text,cs))\n",
    "            else:\n",
    "                s=np.array([1e-5 for x in range(WV_SIZE)])\n",
    "                cs=cosine_sim(input_embedding,s)\n",
    "                if not math.isnan(cs):\n",
    "                    cs_list.append((doc[0].text,cs))\n",
    "                \n",
    "                \n",
    "                    \n",
    "    sorted_cs_list=sorted(cs_list,key=lambda tup:tup[1],reverse=True)\n",
    "    return sorted_cs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_similar_for_lstm(input_embedding):\n",
    "    cs_list=[]\n",
    "    for v in global_vocab:\n",
    "        doc=nlp(v)\n",
    "        if doc[0].has_vector:\n",
    "            if doc[0].vector.all()!=0.0:\n",
    "                cs=cosine_sim(input_embedding,doc[0].vector)\n",
    "                if not math.isnan(cs):\n",
    "                    cs_list.append((doc[0].text,cs))\n",
    "            else:\n",
    "                s=np.array([1e-5 for x in range(WV_SIZE)])\n",
    "                cs=cosine_sim(input_embedding,s)\n",
    "                if not math.isnan(cs):\n",
    "                    cs_list.append((doc[0].text,cs))\n",
    "                \n",
    "                \n",
    "                    \n",
    "    sorted_cs_list=sorted(cs_list,key=lambda tup:tup[1],reverse=True)\n",
    "    return sorted_cs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_list_from_tuple(ltuple):\n",
    "    res=[]\n",
    "    scores=[]\n",
    "    for k,v in ltuple:\n",
    "        res.append(k)\n",
    "        scores.append(v)\n",
    "    return res,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rank_bins(ranks):\n",
    "    #500,1000,5000,10000\n",
    "    \n",
    "    N=len(BIN_VALS)\n",
    "    bins={x:0 for x in BIN_VALS}\n",
    "    for i in ranks:\n",
    "        if i<=500:\n",
    "            bins[500]+=1\n",
    "        elif i<=1000 and i>500:\n",
    "            bins[1000]+=1\n",
    "        elif i<=5000 and i>1000:\n",
    "            bins[5000]+=1\n",
    "        elif i<=10000 and i>5000:\n",
    "            bins[10000]+=1\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Metric 1: % of words present in the top recall_count results\n",
    "#Metric 2: mean rank of the target words\n",
    "\n",
    "def eval_method_with_spacy(test_set,recall_count,method='additive'):\n",
    "    recall_size=0\n",
    "    acc=0\n",
    "    i=0\n",
    "    ranks=[]\n",
    "    for t in test_set:\n",
    "        i+=1\n",
    "        target_word=nlp(unicode(t[0]),'utf-8')\n",
    "        input_sentence=unicode(t[1],'utf-8')\n",
    "        input_embedding=make_cumm_embedding(input_sentence,method)\n",
    "        if input_embedding.all()==0:\n",
    "            continue\n",
    "        r=get_list_from_tuple(get_most_similar(input_embedding))\n",
    "        results=r[0]\n",
    "        scores=r[1]\n",
    "        if target_word[0].text in results:\n",
    "            print target_word[0].text\n",
    "            print \"There\"\n",
    "        print input_sentence,' ',target_word[0].text,'/',results[0],' ',scores[0]\n",
    "        if target_word[0].text in results:\n",
    "            target_rank=results.index(target_word[0].text)\n",
    "            ranks.append(target_rank)\n",
    "            print \"%d/%d done for word %s, rank: %d\"%(i,len(test_set),str(target_word[0].text),target_rank)\n",
    "    print \"Mean rank: %f\"%(float(sum(ranks))/float(len(ranks)))\n",
    "    print get_rank_bins(ranks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
